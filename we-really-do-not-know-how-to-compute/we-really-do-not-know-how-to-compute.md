
## [We Really Don't Know How to Compute!](https://www.youtube.com/watch?v=O3tVctB_VSU)
by Gerry Sussman
at [Strange Loop conference](https://www.thestrangeloop.com/about.html), 2011  

Note: This document includes edited transcript of Sussman's talk with associated slided plus my comments and questions. Document is organized in a way that each section contains Sussman's claims in bold, followed by associated slid and possibly my comments and questions.  

[All slides](slides/slides.pdf)  

Opening remark: We are in a real trouble and have not the foggiest idea how to compute very well. There might some glimmer of hope on the horizon, in number of different direction, some of which I might point out.  

[Opening slide](slides/page_1.pdf)

### Most things are obsolete

Most things we are talking about, even here are obsolete. As I said to somebody at the yesterday's Haskell sessions: this is the most advanced of the obsolete languages.

My comments/questions:
1. This is exactly what Alan Kay was saying for a long time, pointing out that top languages in use today haven't got passed Simula which was one of the great languages of the 60s.
2. Since almost all companies are writing millions of lines of mostly incomprehensible code, this is probably greatest confirmation that languages in general use today simply do not scale. It is also amazing to look back at the greatest artifacts of the 60s and 70s and see with how tiny amount of code they were implemented on machines with wastly less resources. For example, as we can hear from Alan Kay, personal computing as done by his group at PARC was done in only about 10k lines of code. Also Howard Shrobe wrote in one of his papers that OS on Lisp Machines (done by Symbolics) was implemented in only about 1 million LOC written in maybe Common Lisp/CLOS (or some other Lisp variant - Lisp Machine Lisp?). So, with better architecture (both hardware and software) one can do a lot with much less material, or as Alan Kay said so many times - architecture dominates material.

### [Kanizsa’s Triangle Illusion](slides/page_2.pdf)

You now see the triange that is not there. What did you do? I took only 100 or maybe 200 ms, or few tens of neuron times. I do not care how parallel the machine is. I do have the foggiest idea how to write a program that organizes complex process like that in the latency of about 30 or 40 steps. That is the example of something we do not understand how to do.  

### Human genome

And more serious than that, human genome is about a GB. That is instructions for making you from a cell. You are pretty complicated machine. A GB is the same size as the source code appropriate for making say Microsoft Windows. It is not much bigger or smaller, it is somethig like that. It has all the possible things it can do, all possible drivers which might be relevant. It is probably of that size. It is not much bigger, not 3 orders of magnitude bigger. A GB is not that big, but what is interesting is that with the small change you make a cowens? out of the person, so it is flexible too. So we have enourmously flexible language there that we do not have the foggiest idea what it is like. I am not talking about DNA. DNA is like gates, or something. What I am interested is the higher level language that is necessary to do things like that and we certainly do not know how to do that.

### Salamander example

Biologists do nasty experiments on animals. Suppose you took a salamander. Salamander can regrow it's limbs if you cut them off. So all these biologists do nasty experiments. They cut salamander's arm off between the shoulder and elbow, and between the elbow and the hand, flip around that segment and resow it back on. What do you think you get? You get 3 elbows. It grows for the fact that the wrong thing is connected to the wrong thing. That's pretty weird. But that is a clue to something that we should be thinking about. How do you talk about how to make a bazillion - human is 10^12 cells, salamander is probably 10^11 or 10^10 - but the fact is how do you make something big like that grow itself? Little neighbours talking to each other and they say: I am supposed to connect to something like this but it ain't there, so we will make some of that. That is sort of amazing kind of process that is going to be necessary if we want to approach the future in our computing because it is almost impossible to setup the situation in which we know everything that is going to happen in the future.

### [IBM 650](slides/page_3.pdf)

Let me go into the past a bit. When I started computing in 1961 I was computing on the IBM 650 at the Columbia State University. That machine costs half a million bucks. It had 2000 words of 10 decimal digits each of memory on a drum that rotated so that access time was 12.5 ms. 10 ms cycle time. Total amount of memory was probably about 10 KB. For the price of that machine now I can buy 50 thousand PCs each with a GB of RAM (that is 10^4 times as much) and million times faster. So one of the things that has been happening over the years (I am not trying to worry about Moore's law or anything like that). What I am worried about is that all our intuitions from being programmers have come from a time of assuming a kind of scarcity, a world in which computation is somehow expensive, where memory is somehow expensive, where we have to optimize it. Well, there some applications like that, but the answer is that most applications are not like that at all. Right now, memory is free, computing is free. It is our problem to figure out how to use computing like that. And if I have 50 thousand PCs in a network which will probably cost about million (modern) dollars, I can make a very serious computer out of that. It is no longer necesssary to minimize operations, to worry about how much memory we take up. It is very important to minimize latency. That is critical, you have to understand that.  

### [Quote from Huw Evans](slides/page_4.pdf)

The real cost of programming is a cost of programmers. Here is a quote from Mr. Evans made a while ago. It turns out that almost all cost of computing is you, and me. That's a different world. That's the real problem.

### Things people worry about

There is lots of things people worry about. People worry about correctness. That is a problem. Is it a real problem? Maybe, probably not. Most things do not have to work. Look at Google. If it makes a mistake, it does not matter, so long if the next time it gets the right answer, or maybe something close to the right answer, reasonable answer. People worry about security. Well they have no idea how to handle security. We will find out for example that humans manage to survive for about 70 years. We are continuously attacked by the parasites. It must be that there are some bad things we are doing. It is all in that GB of code. You have to think about that. We certainly have no any good ideas how to make things last for very long time by continuously being attacked by mutating parasites.

### Evolvability and modifiability of code 

The other thing that seems really important to me is that we spend all our time modifiying our existing code. The problem is that our code is not adequately evolvable and modifiable to fit the future. In fact, what we want is to make systems that have the property that they are good for things that the designer did not even think of or intend. That is a big problem. That is a real problem I want to after a little bit. The problem is that when we build systems we program ourselves into holes. I have done it a number of times. I have been programming since 1961, and found more ways to screw up that anybody I know. I learned a lot I hope but not an infinite amount. So here is this problem. How do we keep ourselves from programming ourselves into holes. What does that mean? It means that we make decisions early into some process that spread all over our system. The consequences of those decisions are such that the things we want to change later are difficult to change becaue we have to change very large amount of stuff. That is the problem. I want to figure out ways we can organize systems so that the consequences of the decisions we make are not expensive to change. We cannot avoid making decisions, but we can try to figure out ways to minimize the cost of changing decisions we made.

### [Traditional generic arithmetic in Scheme](slides/page_5.pdf)

Let me start with the simple example, one that is so familiar that we all do it these days, certainly in good dynamically typed langages like Lisp or Python or whatever, or good statically typed languages like Haskell or whatever. We always have generic operators, we call it operator overloading and things like that. Lets talk about that for a second because I am going to be very extreme about this. [Here](slides/page_5.pdf) is a traditional arithmetic you will find in Scheme. I can add integer together, I can add integers to rationals, I can make new rationals, I can add integers to reals (floating-point thingies that are very dangerous, nothing brings fear than a floating-point number, numerical analysis - the blackest art I know). We can deal with complex numbers, etc. But, those things are built in. How about dynamically extensible generic operations? Now, we can do that. Of course if you separate compile time and runtime, something weird is happening here. I want to be able to dynamically extend things while my program is running. I want a program to compute up what it is going to do. Remember, in the old days we used to program in the assembly language. It was a great language becaue you could write the next instruction you are going to execute. You could put into the place you are going to go to. It just was. Many interesting programs were written that way and it was very flexible. There is too many fearful people in the audience. I an see a lot of people cringing, but the fact is, you can do it in Lisp also - you can eval something you cons up. That is very useful. 

### [Arithmetic on Functions](slides/page_6.pdf)

I can expand this to the arithmetic on functions. I can add sin and square. I get a new function because that is a classical thing people can do in algebra - if you have 2 functions with the same kind of input, then the sum of those is assumed to be the sum of the outputs. That is for things that are typed procedures with the same number of arguments. I can talk about sum of the squares of sin and cos applied and I get 1. That is an example of simple extension. You have all these nice things. I was wathing somebody talking about scalaz today, and of course you can do that.

### [Symbolic Arithmetic](slides/page_7.pdf)

Then you get to things where Lisp gets things better. There is symbolic algebra. Now, this is also a generic extension. I am not going to explain how it works except that I can tell substraction, multiplication, division, all those things to deal with symbolic entities as well, literal numbers. And what I get is a symbolic expression as a result of such a computation and I can of course ... nice things [that](slides/page_7.pdf) is a fragment of a program that came out. So I can write a program that writes a program, and I can use it. I can tell you a good example of the use of that. It is numerical analysis example - Gear integrator, for Siff equations, differential equations. It turns out there are some polynomials you have to compute up. If you compute them ahead of time you will double your step size. But if you want to be able to make the step size any size you want, you want to be able to compute the polynomial at the moment. So you write something that symbolically does that, then you pass that through compiler as you need it and then you run it and use it, in the same program that is running, integrating along. The point is that you can get essentially fully native code efficiency. I like Lisp because it has uniform representation of programs and data, so that symbolic manipulation produces stuff that I can execute. 

### [Automatic Differentiation](slides/page_8.pdf)

You can stuff that is even more wild. Did you know that automatic differentiation can be done as a generic extension? What you do is create hyperreal numbers, numbers that look like x+Δx, in the same way complex number is real part plus "j" times imaginary part, "i" if you are matematician ("j" if you are electrical engineer). You can say: derivative of the sum of cos and square function applies to x. Now I am using my symbolic part as well, so that these things are mixe together. I can look at something more complicated, derivative of the function of 2 variables, which is a structure of 2 partial derivatives, which is what is should be.

### [Derivative Computation a Generic Extension](slides/page_9.pdf)

I am going to show you a trick about how to do this. This is producing compositions in the sam way you have combinators. So that is useful kind of thing. I actually use this all the time, because I teach a class in advanced classical mechanics. I use computer programming in my class to clarify what we write in equations, because mathematical notations are both awful and it turns out ambiguous. To be perfectly honest, it is impressionistic. When I am talking to you in English it is the same phenomena. Mathematicians are talking to each other because almost all of them are alike. The reason you can understand me because we are almost all identical. Only few bits have to transferred in order to get idea across. Mathematicians are painting the idea in the platonic world of the other matematician's mind by talking to them and they write down basically impressionistic scribbles in the same way my speech is not gramatical, beautiful English. 

### [Advanced Use of Generics](slides/page_10.pdf)

But when we write programs we have to be precise because computers are dumb at this point and as a consequence the great way to show someone the real story I can write things like [this]((slides/page_10.pdf). The definition of the method of computing Lagrange equations from Lagrangian, given configuration space path. For those of you who do not about these things it does not matter, I am not going to dwell on it. That is easy stuff. What if I am dealing with something with the couple of rigid bodies that are wobbly and have all sorts of ... Then I cannot write equations of this board. If you do cellestial mechanics like I have done, sometimes equations are 20 pages long, and it better be done by the computer. In the old days people did it by hand. They got it right most of the time. It is amazing. I cannot do things that people did in 1911 or so. Now, we can do this easily, and it turns out this is very simple because of simple generic extension. I am not trying to say that this solves any important problem. Very careful about that. 

### Little tiny feeling what is it needed to make things more flexible

This is little tiny feeling what is it needed to make things more flexible. Consider the value here. The value is that I am dynamically allowed to change the meaning of all my operators to add new things they can do, dynamically, not at compile time, at runtime. Our programs can produce things which are new way to add, that sort of stuff. And then it can attach this to addition, take it off. It can look like binding. I can bind that idea. That is wonderful. What is it doing? It is giving me tremendous flexibility because the program I wrote to run on a bunch of numbers, with only a little bit of tweaking, suddenly runs on matrices as long as I did not make a mistake of computing something wrong. If I did, I am in trouble. This makes proving theorems about things very hard, in fact maybe impossible. The cost and the benefit are very extreme. I can pay correctness, or proof of correctness, or beleif in correctness, for tremendous flexibility. Now, we worry about the ideas like whether correctness is the essential idea I want. I am not opposed to proofs, I like proofs, I am an old mathematician. The problems is that putting us in the situation, which Mr. Dijkstra got us into, about 30 or 40 years ago, where you are supposed to prove everything to be right before you write a program. Getting yourself in that situation puts us into the world where we make our specifications for parts as tight as possible because it is hard to prove general things, except occassionaly sometimes it it easier. Usually it is harder to prove general theorem about something. You make a very specialized thing that works for particular case. You build this very big tower of stuff, and boy is that brittle, change the problem a little, and it all falls over. We did not learn something from electrical engineering world, the digital abstraction, whereby the inputs to something accept much bigger range than the outputs they are allowed to produce, and that way you get rid of noise at every stage. That is kind of flexibility I am expecting to get. By the way, none of the old stuff ... when you do this kind of thing, generic extensions, it does not mean that the old stuff break, it means that the new stuff is not proved. So, many of the techniques I am advocating make proof much more difficult and perhaps impossible. Now, I am going to be little more extreme. I haven't been extreme yet. This is the beginning of stuff. 

### Everything in the world looks like my 650

I think that one of the problems in the kind of architecture we are assuming. Everything in the world looks like my 650. Occasionally people have made wonderful experiments. GPU is different. The Connection Machine was the experiment which was different. But in the future it is going to be that case that computers are so cheap and so easy to make them the size of the grain of sand with MB of RAM, buy them by the bushel I suppose, you can pour them in your concrete, by your concrete by a mega flop, and have a wall that's smart. So long that you can get power to them, they can do something. That is going to happen. Remember, your cells are pretty smart. They have about a GB of ROM, few bytes of RAM probably, or maybe a few hundred (we do not know how many). But, the seem to talk to each other and do useful things. So, we have to begin to worry about that kind of world. 

### Teaching circuit theory at MIT

I am going to tell you the story that started around 1975 and recently had some advances. It is called propagators. It started when I was teaching my first classes at electrical engineering at MIT in circuit theory, actually the first class was field theory, but then I thought circuit theory class with real wizard of circuitry Paul Penfield. And I observed what we thought to students was not at all what students were actually expected to learn, i.e. what an expert person when presented with the circuit that looks like [that](slides/page_11.pdf) was quite different from what we tell them to write down node equations and there are certain for resistors and capacitors and inductors and transistor which ahs complicated non-linear equation, and you are supposed to grind these equations somehow and solve them to find out what is going on. That is not what a really good engineer does. What really good engineer does is looks at these this at says: let's consider the bias analysis here. That means these capacitors are open circuits, etc. (follows analysis of the circuit). Therefore I can tell you all the bias conditions in this transistor. You can use this to check whether my assumptions are right, and they are. (Follows more circuit analysis ...) Within 5% that is right, and every real engineer does that. That is not the sort of thing that we were teaching the students. So I tried to figure out how can we teach students that sort of thing. Of course, my vision of things is that you write a program and give it to the students to read. What I did is I hired Richard Stallman in 1975 who worked for me and he and I wrote that program together. It was quite a success. 

### Modern version of the program for circuits

I will show you a [modern version](slides/page_12.pdf) of it. The old code does not seems to exist anymore. I went looking for it, it was on the old ITS operating system, and it is gone, probably on some backup tape somebody has it can be found but it is not around now. Here is a new version of the same program. Here is a representation of that amplifier. I am not going to read it to you. It is a wiring diagram. The same lambda calculus combination structure that you use for functions works for other kinds of things too. All that it is required is consistent naming scheme and says: let Rb1 be a resistor, etc. Wiring diagram - description of how these things are put together. That is a really nice thing. Remember, underneath this is lambda calculus, nothing more. There is no more magic in there. It is just naming. As one very smart mistic ones said: if you have name of the spirit, you have power over it. So then, once you that, then you can for example make test for this that looks like [that](slides/page_13.pdf). And I can tell it what kinds of parameters I want. Notice that I am allowed to use ... all my values are also allowed to have units. That is another generic extension. It does not matter. It is just simple - once you have generic operators you can have units and it works just great. I can make various other assumptions ... And I can ask what is the potential the base of the amplifier in the bias region (show on this [slide](slides/page_14.pdf); (value (the potential b amp bias)) expression). And it does some deduction and comes up with the answer. The really interesting thing is [here](slides/page_15.pdf). I can ask why?: Mr. Computer why do you believe think that the potential of the amplifier in the bias region is what it is. And the outcome is a remarkable QED that has nothing to do with proving. It is just to be arrogant. What is telling is ... because remember all those things are lies anyway. The model for the transistor I told you is a lie. All of physics and EE is based on useful lies which are appropriate approximations for particular jobs. When Galileo invented the modern physics, for all practical purposes what he discovered is the value of a lie. Aristotle understood when you roll the ball on the floor always stopped. Galileo said forget about the friction, lets figure it out, and than put the friction back in. That was a required lie to understand what was really going on. That changed the world. That was the big breakthrough of Galileo. So, this QED does not mean anything.  

### [The Propagator Idea](slides/page_16.pdf)

What really matters is that I am chasing the provenance of every value. As the values move around in this thing ... and what is really going on here is I am doing what the expert did. I am seeing something very obvious. What I am seeing is that if I know the value here then I know the value here becaue of local phenomena. I am propagating information around. Information is propagating around. We call this propagators. That was a big idea in 1975. However it does have some power. Imagine that propagators are independent little statless machines that are connected to the number of these little things called cells (not biological). Information move around and it is allowed to go in any direction. So there are the statful cells that have state and there are these propagator machines. Guess what, this very nice for very parallel machine. But even better, Alexey Radul, my graduate student, two years ago got his doctor's degree, made a breakthrough here. The great breakthrough was that we do not actually put values in these cells, we put information about a value. I will get to that a little later. What it means is that what the cell contains is monotonically increasing information about a value. It gets better and better information and as a consequence things like synchronization problems go away, for a parallel machine. This becomes a very parallel mechanism for building a very large complex machine in a simple way. But it is electrical engineer's point of view, it is not a computer science point of view. It is not dataflow. It is closely related, but it is not. It is a very different kind of thing because we are worried about information that is being accumulated in these cells. 

### [Expressions have Anonymous Connections](slides/page_17.pdf)

But let me go a little furher. What are the other problems? I hate computer languages now, almost all of them, including those I invented. And part of the reason is becaue they are full of expressions. It could be worse, it could be statements. But they are expressions. And what I mean by that is ... expression is a tree. You got a bunch of values and they only go up the tree and they are collected in one output. That is what we want to think about. And you do not have the names for the intermediate parts, as in the circuit diagram of the electrical circuit. Those are useful places to put names on. Sometimes they are pain in the butt. You have to be careful not to get overwhelmed with names. One difference in [this](slides/page_18.pdf) ([here](slides/page_19.pdf) is it's usage) case, I am going to change it so in propagators all connections are explicit, they have names. That  is a computation for the square root by Heron's method. It is a wiring diagram. I am looking at something that looks like an assembly language. I do not have a good language for this yet, but I am using lambda glue to put it together. Lambda glue is excelent for getting going before we understand what the real language is. Each of these things are autonomous machines running continuously. If I can burn processor I can do that. Suppose I have as much processor as I have memory. Suppose that for every memory module I have big processor associated with it. That will happen. Because there is no other way to make future happen. You can also make it [iterative](slides/page_20.pdf). Wiring diagram view of the world. What we are seeing is a wiring diagram for the machine. Machine is potentially infinite, becaues it has the copy of itself inside of itself. This is the way to think about things which is quite different. These things might have multiple outputs. For example, when you divide integers, you get a quotient and the remainder. Why would I get one or the other? Why not even think about that as a normal way to think? Most processes have that property. What if you have things which have many things going in and many go out? The way square-iter knows whether or not to expand itself, that is the implementation question. This is the question between applicative order and normal order in lamba calculus. Am I choosing to expand something when all inputs are there or only one input or you want the ouput, or anything like that ... that is a different question. I am not going to worry about that right now. We can make machines do it anyway we like and we could have policies that could be allowed to both. Remember, a real engineer does not want just a religion on how to solve the problem, like object-oriented or functional or imperative or logic programming. This piece of the problem wants to be functional program, this piece wants to be imperative, etc. and they all want to work together usefully. And because the way the problem is structured, whatever it is. I do not want to think there is any correct answer to any of those questions. It would be awful bad writing device driver in a functional language. It would be bad to writing symbolic manipulator with the thing with complicated syntax. It is bad to write numerical program in anything but Fortran. That is a joke. Fortran is a wonderful language in some ways. It is ancestor of them all, besides Lisp. Fortran and Lisp are 2 oldest languages that are still in use. I think they are the keys to everything else, current version of langages. 

What can you do with these propagators?
