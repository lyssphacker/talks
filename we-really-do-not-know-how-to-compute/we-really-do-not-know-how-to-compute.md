
## [We Really Don't Know How to Compute!](https://www.youtube.com/watch?v=O3tVctB_VSU)
by Gerry Sussman
at [Strange Loop conference](https://www.thestrangeloop.com/about.html), 2011  

Note: This document includes edited transcript of Sussman's talk with associated slided plus my comments and questions. Document is organized in a way that each section contains Sussman's claims in bold, followed by associated slid and possibly my comments and questions.  

[All slides](slides/slides.pdf)  

Opening remark: We are in a real trouble and have not the foggiest idea how to compute very well. There might some glimmer of hope on the horizon, in number of different direction, some of which I might point out.  

[Opening slide](slides/page_1.pdf)

### Most things are obsolete

Most things we are talking about, even here are obsolete. As I said to somebody at the yesterday's Haskell sessions: this is the most advanced of the obsolete languages.

My comments/questions:
1. This is exactly what Alan Kay was saying for a long time, pointing out that top languages in use today haven't got passed Simula which was one of the great languages of the 60s.
2. Since almost all companies are writing millions of lines of mostly incomprehensible code, this is probably greatest confirmation that languages in general use today simply do not scale. It is also amazing to look back at the greatest artifacts of the 60s and 70s and see with how tiny amount of code they were implemented on machines with wastly less resources. For example, as we can hear from Alan Kay, personal computing as done by his group at PARC was done in only about 10k lines of code. Also Howard Shrobe wrote in one of his papers that OS on Lisp Machines (done by Symbolics) was implemented in only about 1 million LOC written in maybe Common Lisp/CLOS (or some other Lisp variant - Lisp Machine Lisp?). So, with better architecture (both hardware and software) one can do a lot with much less material, or as Alan Kay said so many times - architecture dominates material.

### [Kanizsa’s Triangle Illusion](slides/page_2.pdf)

You now see the triange that is not there. What did you do? I took only 100 or maybe 200 ms, or few tens of neuron times. I do not care how parallel the machine is. I do have the foggiest idea how to write a program that organizes complex process like that in the latency of about 30 or 40 steps. That is the example of something we do not understand how to do.  

### Human genome

And more serious than that, human genome is about a GB. That is instructions for making you from a cell. You are pretty complicated machine. A GB is the same size as the source code appropriate for making say Microsoft Windows. It is not much bigger or smaller, it is somethig like that. It has all the possible things it can do, all possible drivers which might be relevant. It is probably of that size. It is not much bigger, not 3 orders of magnitude bigger. A GB is not that big, but what is interesting is that with the small change you make a cowens? out of the person, so it is flexible too. So we have enourmously flexible language there that we do not have the foggiest idea what it is like. I am not talking about DNA. DNA is like gates, or something. What I am interested is the higher level language that is necessary to do things like that and we certainly do not know how to do that.

### Salamander example

Biologists do nasty experiments on animals. Suppose you took a salamander. Salamander can regrow it's limbs if you cut them off. So all these biologists do nasty experiments. They cut salamander's arm off between the shoulder and elbow, and between the elbow and the hand, flip around that segment and resow it back on. What do you think you get? You get 3 elbows. It grows for the fact that the wrong thing is connected to the wrong thing. That's pretty weird. But that is a clue to something that we should be thinking about. How do you talk about how to make a bazillion - human is 10^12 cells, salamander is probably 10^11 or 10^10 - but the fact is how do you make something big like that grow itself? Little neighbours talking to each other and they say: I am supposed to connect to something like this but it ain't there, so we will make some of that. That is sort of amazing kind of process that is going to be necessary if we want to approach the future in our computing because it is almost impossible to setup the situation in which we know everything that is going to happen in the future.

### [IBM 650](slides/page_3.pdf)

Let me go into the past a bit. When I started computing in 1961 I was computing on the IBM 650 at the Columbia State University. That machine costs half a million bucks. It had 2000 words of 10 decimal digits each of memory on a drum that rotated so that access time was 12.5 ms. 10 ms cycle time. Total amount of memory was probably about 10 KB. For the price of that machine now I can buy 50 thousand PCs each with a GB of RAM (that is 10^4 times as much) and million times faster. So one of the things that has been happening over the years (I am not trying to worry about Moore's law or anything like that). What I am worried about is that all our intuitions from being programmers have come from a time of assuming a kind of scarcity, a world in which computation is somehow expensive, where memory is somehow expensive, where we have to optimize it. Well, there some applications like that, but the answer is that most applications are not like that at all. Right now, memory is free, computing is free. It is our problem to figure out how to use computing like that. And if I have 50 thousand PCs in a network which will probably cost about million (modern) dollars, I can make a very serious computer out of that. It is no longer necesssary to minimize operations, to worry about how much memory we take up. It is very important to minimize latency. That is critical, you have to understand that.  

### [Quote from Huw Evans](slides/page_4.pdf)

The real cost of programming is a cost of programmers. Here is a quote from Mr. Evans made a while ago. It turns out that almost all cost of computing is you, and me. That's a different world. That's the real problem.

### Things people worry about

There is lots of things people worry about. People worry about correctness. That is a problem. Is it a real problem? Maybe, probably not. Most things do not have to work. Look at Google. If it makes a mistake, it does not matter, so long if the next time it gets the right answer, or maybe something close to the right answer, reasonable answer. People worry about security. Well they have no idea how to handle security. We will find out for example that humans manage to survive for about 70 years. We are continuously attacked by the parasites. It must be that there are some bad things we are doing. It is all in that GB of code. You have to think about that. We certainly have no any good ideas how to make things last for very long time by continuously being attacked by mutating parasites.

### Evolvability and modifiability of code 

The other thing that seems really important to me is that we spend all our time modifiying our existing code. The problem is that our code is not adequately evolvable and modifiable to fit the future. In fact, what we want is to make systems that have the property that they are good for things that the designer did not even think of or intend. That is a big problem. That is a real problem I want to after a little bit. The problem is that when we build systems we program ourselves into holes. I have done it a number of times. I have been programming since 1961, and found more ways to screw up that anybody I know. I learned a lot I hope but not an infinite amount. So here is this problem. How do we keep ourselves from programming ourselves into holes. What does that mean? It means that we make decisions early into some process that spread all over our system. The consequences of those decisions are such that the things we want to change later are difficult to change becaue we have to change very large amount of stuff. That is the problem. I want to figure out ways we can organize systems so that the consequences of the decisions we make are not expensive to change. We cannot avoid making decisions, but we can try to figure out ways to minimize the cost of changing decisions we made.

### [Traditional generic arithmetic in Scheme](slides/page_5.pdf)

Let me start with the simple example, one that is so familiar that we all do it these days, certainly in good dynamically typed langages like Lisp or Python or whatever, or good statically typed languages like Haskell or whatever. We always have generic operators, we call it operator overloading and things like that. Lets talk about that for a second because I am going to be very extreme about this. [Here](slides/page_5.pdf) is a traditional arithmetic you will find in Scheme. I can add integer together, I can add integers to rationals, I can make new rationals, I can add integers to reals (floating-point thingies that are very dangerous, nothing brings fear than a floating-point number, numerical analysis - the blackest art I know). We can deal with complex numbers, etc. But, those things are built in. How about dynamically extensible generic operations? Now, we can do that. Of course if you separate compile time and runtime, something weird is happening here. I want to be able to dynamically extend things while my program is running. I want a program to compute up what it is going to do. Remember, in the old days we used to program in the assembly language. It was a great language becaue you could write the next instruction you are going to execute. You could put into the place you are going to go to. It just was. Many interesting programs were written that way and it was very flexible. There is too many fearful people in the audience. I an see a lot of people cringing, but the fact is, you can do it in Lisp also - you can eval something you cons up. That is very useful. 

### [Arithmetic on Functions](slides/page_6.pdf)

I can expand this to the arithmetic on functions. I can add sin and square. I get a new function because that is a classical thing people can do in algebra - if you have 2 functions with the same kind of input, then the sum of those is assumed to be the sum of the outputs. That is for things that are typed procedures with the same number of arguments. I can talk about sum of the squares of sin and cos applied and I get 1. That is an example of simple extension. You have all these nice things. I was wathing somebody talking about scalaz today, and of course you can do that.

### [Symbolic Arithmetic](slides/page_7.pdf)

Then you get to things where Lisp gets things better. There is symbolic algebra. Now, this is also a generic extension. I am not going to explain how it works except that I can tell substraction, multiplication, division, all those things to deal with symbolic entities as well, literal numbers. And what I get is a symbolic expression as a result of such a computation and I can of course ... nice things [that](slides/page_7.pdf) is a fragment of a program that came out. So I can write a program that writes a program, and I can use it. I can tell you a good example of the use of that. It is numerical analysis example - Gear integrator, for Siff equations, differential equations. It turns out there are some polynomials you have to compute up. If you compute them ahead of time you will double your step size. But if you want to be able to make the step size any size you want, you want to be able to compute the polynomial at the moment. So you write something that symbolically does that, then you pass that through compiler as you need it and then you run it and use it, in the same program that is running, integrating along. The point is that you can get essentially fully native code efficiency. I like Lisp because it has uniform representation of programs and data, so that symbolic manipulation produces stuff that I can execute. 

### [Automatic Differentiation](slides/page_8.pdf)

You can stuff that is even more wild. Did you know that automatic differentiation can be done as a generic extension? What you do is create hyperreal numbers, numbers that look like x+Δx, in the same way complex number is real part plus "j" times imaginary part, "i" if you are matematician ("j" if you are electrical engineer). You can say: derivative of the sum of cos and square function applies to x. Now I am using my symbolic part as well, so that these things are mixe together. I can look at something more complicated, derivative of the function of 2 variables, which is a structure of 2 partial derivatives, which is what is should be.

### [Derivative Computation a Generic Extension](slides/page_9.pdf)

I am going to show you a trick about how to do this. This is producing compositions in the sam way you have combinators. So that is useful kind of thing. I actually use this all the time, because I teach a class in advanced classical mechanics. I use computer programming in my class to clarify what we write in equations, because mathematical notations are both awful and it turns out ambiguous. To be perfectly honest, it is impressionistic. When I am talking to you in English it is the same phenomena. Mathematicians are talking to each other because almost all of them are alike. The reason you can understand me because we are almost all identical. Only few bits have to transferred in order to get idea across. Mathematicians are painting the idea in the platonic world of the other matematician's mind by talking to them and they write down basically impressionistic scribbles in the same way my speech is not gramatical, beautiful English. 

### [Advanced Use of Generics](slides/page_10.pdf)

But when we write programs we have to be precise because computers are dumb at this point and as a consequence the great way to show someone the real story I can write things like [this]((slides/page_10.pdf). The definition of the method of computing Lagrange equations from Lagrangian, given configuration space path. For those of you who do not about these things it does not matter, I am not going to dwell on it. That is easy stuff. What if I am dealing with something with the couple of rigid bodies that are wobbly and have all sorts of ... Then I cannot write equations of this board. If you do cellestial mechanics like I have done, sometimes equations are 20 pages long, and it better be done by the computer. In the old days people did it by hand. They got it right most of the time. It is amazing. I cannot do things that people did in 1911 or so. Now, we can do this easily, and it turns out this is very simple because of simple generic extension. I am not trying to say that this solves any important problem. Very careful about that. 

### Little tiny feeling what is it needed to make things more flexible

This is little tiny feeling what is it needed to make things more flexible. Consider the value here. The value is that I am dynamically allowed to change the meaning of all my operators to add new things they can do, dynamically, not at compile time, at runtime. Our programs can produce things which are new way to add, that sort of stuff. And then it can attach this to addition, take it off. It can look like binding. I can bind that idea. That is wonderful. What is it doing? It is giving me tremendous flexibility because the program I wrote to run on a bunch of numbers, with only a little bit of tweaking, suddenly runs on matrices as long as I did not make a mistake of computing something wrong. If I did, I am in trouble. This makes proving theorems about things very hard, in fact maybe impossible. The cost and the benefit are very extreme. I can pay correctness, or proof of correctness, or beleif in correctness, for tremendous flexibility. Now, we worry about the ideas like whether correctness is the essential idea I want. I am not opposed to proofs, I like proofs, I am an old mathematician. The problems is that putting us in the situation, which Mr. Dijkstra got us into, about 30 or 40 years ago, where you are supposed to prove everything to be right before you write a program. Getting yourself in that situation puts us into the world where we make our specifications for parts as tight as possible because it is hard to prove general things, except occassionaly sometimes it it easier. Usually it is harder to prove general theorem about something. You make a very specialized thing that works for particular case. You build this very big tower of stuff, and boy is that brittle, change the problem a little, and it all falls over. We did not learn something from electrical engineering world, the digital abstraction, whereby the inputs to something accept much bigger range than the outputs they are allowed to produce, and that way you get rid of noise at every stage. That is kind of flexibility I am expecting to get. By the way, none of the old stuff ... when you do this kind of thing, generic extensions, it does not mean that the old stuff break, it means that the new stuff is not proved. So, many of the techniques I am advocating make proof much more difficult and perhaps impossible. Now, I am going to be little more extreme. I haven't been extreme yet. This is the beginning of stuff. 

### Everything in the world looks like my 650

I think that one of the problems in the kind of architecture we are assuming. Everything in the world looks like my 650. Occasionally people have made wonderful experiments. GPU is different. The Connection Machine was the experiment which was different. But in the future it is going to be that case that computers are so cheap and so easy to make them the size of the grain of sand with MB of RAM, buy them by the bushel I suppose, you can pour them in your concrete, by your concrete by a mega flop, and have a wall that's smart. So long that you can get power to them, they can do something. That is going to happen. Remember, your cells are pretty smart. They have about a GB of ROM, few bytes of RAM probably, or maybe a few hundred (we do not know how many). But, the seem to talk to each other and do useful things. So, we have to begin to worry about that kind of world. 


